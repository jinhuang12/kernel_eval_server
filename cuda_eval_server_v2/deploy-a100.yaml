# A100 GPU Deployment for CUDA Evaluation Server
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cuda-eval-server-v2-a100
  namespace: hyperpod-ns-silverhand
  labels:
    app: cuda-eval-server-v2
    gpu-type: a100
    kueue.x-k8s.io/queue-name: hyperpod-ns-silverhand-localqueue
spec:
  replicas: 2
  selector:
    matchLabels:
      app: cuda-eval-server-v2
      gpu-type: a100
  template:
    metadata:
      labels:
        app: cuda-eval-server-v2
        gpu-type: a100
        kueue.x-k8s.io/queue-name: hyperpod-ns-silverhand-localqueue
    spec:
      nodeSelector:
        node.kubernetes.io/instance-type: ml.p4d.24xlarge
      # Tolerations allow pods to schedule on "tainted" nodes
      # GPU nodes often have taints to prevent non-GPU workloads
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists  # Matches any value for this key
        effect: NoSchedule  # Allows scheduling despite NoSchedule taint
      # Volumes define storage that can be mounted into containers
      volumes:
        # Shared memory is crucial for multi-process GPU apps (PyTorch DataLoader, NCCL)
        # Without this, defaults to 64MB which causes "out of shared memory" errors
        - name: shmem
          hostPath:  # Mounts a path from the host node
            path: /dev/shm  # Linux shared memory location
            type: Directory  # Must already exist on host
        
        # NVMe SSDs on GPU instances provide fast local storage
        # Much faster than EBS for temporary data
        - name: local-nvme
          hostPath:
            path: /mnt/k8s-disks/0  # Standard path for instance store on EKS
            type: DirectoryOrCreate  # Creates if doesn't exist
        
        # FSx for Lustre provides high-performance shared storage
        # Optional - remove if you don't have FSx configured
        - name: fsx-storage
          persistentVolumeClaim:  # References a PVC you must create separately
            claimName: fsx-claim  # Name of the PersistentVolumeClaim 
      containers:
      - name: cuda-eval-server-v2
        image: 592892253131.dkr.ecr.us-east-1.amazonaws.com/cuda-eval-server:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 8000
        env:
          # CUDA configuration - controls GPU visibility
          - name: NVIDIA_DRIVER_CAPABILITIES
            value: "compute,utility"  # GPU compute and nvidia-smi
          # Python configuration
          - name: PYTHONUNBUFFERED
            value: "1"  # Ensures logs appear immediately (not buffered)
          - name: PYTHONPATH
            value: "/app:/app/KernelBench/scripts/cuda_eval_server_v2"  # Module search paths
          # NCU (NVIDIA Nsight Compute) profiling configuration
          - name: ENABLE_DEVICE_METRICS
            value: "true"  # Enable detailed GPU profiling
          # Server configuration
          - name: LOG_LEVEL
            value: "info"  # Logging verbosity (debug/info/warning/error)
          # Pod information injected from Kubernetes metadata
          # Useful for debugging and logging
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name  # Actual pod name (e.g., cuda-eval-server-v2-5d7b9c4d5-x2n4j)
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace  # Kubernetes namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName  # Which EC2 instance pod is on      
        resources:
          requests:
            nvidia.com/gpu: "8"
            cpu: "48"
            memory: "576Gi"
            vpc.amazonaws.com/efa: "4"
          limits:
            nvidia.com/gpu: "8"
            cpu: "80"
            memory: "960Gi"
            vpc.amazonaws.com/efa: "4"
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 15
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 15
          periodSeconds: 30
                  # Volume mounts connect volumes to container filesystem
        volumeMounts:
          # Shared memory mount - critical for PyTorch/CUDA
          - name: shmem
            mountPath: /dev/shm  # Standard Linux shared memory path
          
          # Instance store NVMe - fastest available storage
          - name: local-nvme
            mountPath: /local  # Your app can use /local for temp files
          
          # FSx mount for shared data between pods
          - name: fsx-storage
            mountPath: /fsx  # Access shared filesystem at /fsx
        
        # Security context sets container security policies
        securityContext:
          allowPrivilegeEscalation: false  # Don't allow gaining privileges
          runAsNonRoot: false  # NCU profiling requires root access
          runAsUser: 0  # UID 0 = root (needed for GPU profiling)
          seccompProfile:
            type: Unconfined # Allows for NCU 
          capabilities:
            # Linux capabilities are fine-grained permissions
            add:
              - SYS_ADMIN  # Required for NCU to access GPU performance counters
              - SYS_PTRACE  # Required for profiling other processes  

---
apiVersion: v1
kind: Service
metadata:
  name: cuda-eval-service-v2-a100
  namespace: hyperpod-ns-silverhand
  labels:
    app: cuda-eval-server-v2
    gpu-type: a100
spec:
  selector:
    app: cuda-eval-server-v2
    gpu-type: a100
  ports:
  - port: 8000
    targetPort: 8000
  type: ClusterIP