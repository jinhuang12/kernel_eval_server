"""
Shared data models for CUDA Evaluation Server V2
Enhanced with user-definable input/output specifications
"""

from abc import ABC, abstractmethod
from pydantic import BaseModel
from dataclasses import dataclass, field, asdict
from typing import Dict, List, Optional, Any, Union
from enum import Enum
import time
import torch

from shared.kernel_metadata import KernelMetadata, BaseKernelMetadata, CudaKernelMetadata, TorchKernelMetadata, TritonKernelMetadata, MultiKernelMetadata

# ---------------------------------------------------------------------
# Utilities
# ---------------------------------------------------------------------

def _strip_none(d: Dict[str, Any]) -> Dict[str, Any]:
    return {k: v for k, v in d.items() if v is not None}

def _enum_val(x):
    return x.value if isinstance(x, Enum) else x

def _as_list(x):
    return list(x) if isinstance(x, (list, tuple)) else x

def _round_float_values(obj: Any, decimal_places: int = 3) -> Any:
    """
    Recursively round all float values in nested dictionaries/lists to specified decimal places.

    Args:
        obj: Object to process (dict, list, float, or other)
        decimal_places: Number of decimal places to round to (default: 3)

    Returns:
        Object with all float values rounded
    """
    if isinstance(obj, dict):
        return {k: _round_float_values(v, decimal_places) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [_round_float_values(item, decimal_places) for item in obj]
    elif isinstance(obj, float):
        return round(obj, decimal_places)
    else:
        return obj


# Kernel Type Enum
class KernelType(str, Enum):
    """Kernel implementation types"""
    TORCH = "torch"           # Pure PyTorch (reference models)
    CUDA = "cuda"            # Raw CUDA kernels
    TRITON = "triton"        # Triton kernels
    TORCH_CUDA = "torch_cuda"  # PyTorch with embedded CUDA (current generated code)
    MULTI_KERNEL = "multi_kernel"  # Python scripts with mixed kernel types


@dataclass
class TensorData:
    """Literal tensor payload sent over JSON (optional)"""
    # Raw storage buffer (row-major unless 'strides' given)
    data_b64: str                  # base64 of raw bytes
    # Required to reconstruct
    dtype: str                     # e.g. "float32"
    shape: List[int] = field(default_factory=list)
    compress: str = "none"         # "none" | "zlib"

    def to_dict(self) -> Dict[str, Any]:
        return _strip_none({
            "data_b64": self.data_b64,
            "compress": self.compress,
            "shape": self.shape,
            "dtype": self.dtype,
        })

    @classmethod
    def from_dict(cls, d: Dict[str, Any]) -> "TensorData":
        return cls(
            data_b64=d["data_b64"],
            compress=d.get("compress", "none"),
            shape=[int(x) for x in d.get("shape", [])],
            dtype=d.get("dtype", "float32")
        )
    

@dataclass
class TensorInit:
    """How to generate a tensor deterministically on the server (if no data payload)."""
    kind: str = "randn"            # "randn" | "zeros" | "ones" | "uniform" | "full" | "arange"
    seed: Optional[int] = None
    mean: Optional[float] = None   # randn
    std: Optional[float] = None    # randn
    low: Optional[float] = None    # uniform
    high: Optional[float] = None   # uniform
    fill_value: Optional[float] = None  # full
    start: Optional[float] = None  # arange
    step: Optional[float] = None   # arange

    def to_dict(self) -> Dict[str, Any]:
        return _strip_none({
            "kind": self.kind,
            "seed": self.seed,
            "mean": self.mean,
            "std": self.std,
            "low": self.low,
            "high": self.high,
            "fill_value": self.fill_value,
            "start": self.start,
            "step": self.step,
        })

    @classmethod
    def from_dict(cls, d: Dict[str, Any]) -> "TensorInit":
        return cls(
            kind=d.get("kind", "randn"),
            seed=d.get("seed"),
            mean=d.get("mean"),
            std=d.get("std"),
            low=d.get("low"),
            high=d.get("high"),
            fill_value=d.get("fill_value"),
            start=d.get("start"),
            step=d.get("step"),
        )


@dataclass
class TensorSpec:
    """Specification for a tensor argument or output"""
    shape: List[int] = field(default_factory=list) # required when init is used; can be omitted if data provided
    dtype: str = "float32"
    init: Optional[TensorInit] = None    # generate on server
    data: Optional[TensorData] = None    # OR literal payload (mutually exclusive with init)
    
    def to_dict(self) -> Dict[str, Any]:
        return _strip_none({
            "shape": [int(x) for x in self.shape],
            "dtype": self.dtype,
            "init": self.init.to_dict() if self.init else None,
            "data": self.data.to_dict() if self.data else None,
        })

    @classmethod
    def from_dict(cls, d: Dict[str, Any]) -> "TensorSpec":
        return cls(
            shape=[int(x) for x in d["shape"]],
            dtype=d.get("dtype", "float32"),
            init=TensorInit.from_dict(d["init"]) if d.get("init") else None,
            data=TensorData.from_dict(d["data"]) if d.get("data") else None,
        )

@dataclass
class LaunchDim:
    x: int = 1
    y: int = 1
    z: int = 1

    def to_dict(self) -> Dict[str, Any]:
        return {"x": int(self.x), "y": int(self.y), "z": int(self.z)}

    @classmethod
    def from_dict(cls, d: Dict[str, Any]) -> "LaunchDim":
        return cls(x=int(d.get("x", 1)), y=int(d.get("y", 1)), z=int(d.get("z", 1)))

@dataclass
class LaunchConfig:
    grid: Optional[LaunchDim] = None
    block: Optional[LaunchDim] = None          # CUDA-only
    num_warps: Optional[int] = None            # Triton-only
    num_stages: Optional[int] = None           # Triton-only

    def to_dict(self) -> Dict[str, Any]:
        return _strip_none({
            "grid": self.grid.to_dict() if self.grid else None,
            "block": self.block.to_dict() if self.block else None,
            "num_warps": self.num_warps,
            "num_stages": self.num_stages,
        })

    @classmethod
    def from_dict(cls, d: Dict[str, Any]) -> "LaunchConfig":
        return cls(
            grid=LaunchDim.from_dict(d["grid"]) if d.get("grid") else None,
            block=LaunchDim.from_dict(d["block"]) if d.get("block") else None,
            num_warps=int(d["num_warps"]) if d.get("num_warps") is not None else None,
            num_stages=int(d["num_stages"]) if d.get("num_stages") is not None else None,
        )

@dataclass
class ArgSpec:
    """Kernel argument specification"""
    name: str
    type: str  # "tensor","int","float","str","bool"
    value: Optional[Union[int, float, str, bool]] = None
    tensor_spec: Optional[TensorSpec] = None
    role: str = "input"            # "input" | "output" | "inout"
    is_meta: bool = False          # Triton constexpr/meta

    def to_dict(self) -> Dict[str, Any]:
        return _strip_none({
            "name": self.name,
            "type": self.type,
            "value": self.value,
            "tensor_spec": self.tensor_spec.to_dict() if self.tensor_spec else None,
            "role": self.role,
            "is_meta": self.is_meta,
        })

    @classmethod
    def from_dict(cls, d: Dict[str, Any]) -> "ArgSpec":
        return cls(
            name=d["name"],
            type=d["type"],
            value=d.get("value"),
            tensor_spec=TensorSpec.from_dict(d["tensor_spec"]) if d.get("tensor_spec") else None,
            role=d.get("role", "input"),
            is_meta=bool(d.get("is_meta", False)),
        )


@dataclass
class IOContract:
    args: List[ArgSpec]
    launch: Optional[LaunchConfig] = None

    def to_dict(self) -> Dict[str, Any]:
        return _strip_none({
            "args": [a.to_dict() for a in self.args],
            "launch": self.launch.to_dict() if self.launch else None,
        })

    @classmethod
    def from_dict(cls, d: Dict[str, Any]) -> "IOContract":
        return cls(
            args=[ArgSpec.from_dict(x) for x in d.get("args", [])],
            launch=LaunchConfig.from_dict(d["launch"]) if d.get("launch") else None,
        )


# Kernel Code Wrapper
@dataclass
class KernelCode:
    """Wrapper for kernel source code with type information"""
    source_code: str
    kernel_type: KernelType
    io: Optional[IOContract] = None
    # Metadata can be typed dataclass or dict for backward compatibility
    metadata: Optional[BaseKernelMetadata] = None

    def to_dict(self) -> Dict[str, Any]:
        # Convert metadata to dict if it's a Pydantic model
        metadata_dict = None
        if self.metadata:
            if isinstance(self.metadata, BaseKernelMetadata):
                # Convert Pydantic model to dict (use model_dump for Pydantic v2)
                metadata_dict = self.metadata.model_dump()
            else:
                # Already a dict or other type
                metadata_dict = self.metadata
        
        return _strip_none({
            "source_code": self.source_code,
            "kernel_type": _enum_val(self.kernel_type),
            "metadata": metadata_dict,
            "io": self.io.to_dict() if self.io else None,
        })

    def get_typed_metadata(self) -> Optional[BaseKernelMetadata]:
        """Get metadata as typed dataclass if possible"""
        if isinstance(self.metadata, BaseKernelMetadata):
            return self.metadata
        elif isinstance(self.metadata, dict):
            if self.kernel_type == KernelType.CUDA:
                return CudaKernelMetadata.from_dict(self.metadata)
            elif self.kernel_type == KernelType.TORCH:
                return TorchKernelMetadata.from_dict(self.metadata)
            elif self.kernel_type == KernelType.TRITON:
                return TritonKernelMetadata.from_dict(self.metadata)
            elif self.kernel_type == KernelType.MULTI_KERNEL:
                return MultiKernelMetadata.from_dict(self.metadata)
        return None
    
    @classmethod
    def from_dict(cls, d: Dict[str, Any]) -> "KernelCode":
        kt = d.get("kernel_type")
        kt = KernelType(kt) if not isinstance(kt, KernelType) else kt
        return cls(
            source_code=d["source_code"],
            kernel_type=kt,
            metadata=d.get("metadata"),
            io=IOContract.from_dict(d["io"]) if d.get("io") else None,
        )


# Response Data Models
@dataclass
class RuntimeStats:
    """Runtime statistics from kernel profiling"""
    mean: float
    std: float
    min: float
    max: float
    median: float
    percentile_95: float = 0.0
    percentile_99: float = 0.0
    
    @classmethod
    def from_dict(cls, d: Dict[str, float]) -> "RuntimeStats":
        """Create from dictionary, handling missing fields"""
        return cls(
            mean=d.get("mean", 0.0),
            std=d.get("std", 0.0),
            min=d.get("min", 0.0),
            max=d.get("max", 0.0),
            median=d.get("median", 0.0),
            percentile_95=d.get("percentile_95", 0.0),
            percentile_99=d.get("percentile_99", 0.0)
        )


@dataclass
class SpeedOfLightMetrics:
    """Speed of Light performance metrics"""
    compute_memory_throughput_pct: Optional[float] = None
    compute_throughput_pct: Optional[float] = None
    sm_throughput_pct: Optional[float] = None
    gpu_dram_throughput_pct: Optional[float] = None
    memory_throughput_pct: Optional[float] = None
    dram_throughput_pct: Optional[float] = None


@dataclass  
class DetailedMetrics:
    """Detailed performance metrics"""
    l1_hit_rate_pct: Optional[float] = None
    l2_hit_rate_pct: Optional[float] = None
    warp_occupancy_pct: Optional[float] = None
    sm_active_cycles_pct: Optional[float] = None
    instructions_per_cycle: Optional[float] = None
    waves_per_sm: Optional[float] = None


@dataclass
class MemoryMetrics:
    """Memory hierarchy performance metrics"""
    dram_avg_bandwidth_gb_s: Optional[float] = None
    dram_total_bandwidth_gb_s: Optional[float] = None
    dram_active_cycles_pct: Optional[float] = None
    l1_writeback_active_pct: Optional[float] = None
    l1_read_sectors_pct: Optional[float] = None
    l2_throughput_pct: Optional[float] = None


@dataclass
class ComputeMetrics:
    """Compute utilization metrics"""
    fma_pipe_utilization_pct: Optional[float] = None
    fp64_pipe_utilization_pct: Optional[float] = None
    alu_pipe_utilization_pct: Optional[float] = None
    xu_pipe_utilization_pct: Optional[float] = None
    tensor_pipe_utilization_pct: Optional[float] = None
    instructions_per_cycle: Optional[float] = None
    occupancy_limit_blocks: Optional[float] = None
    occupancy_limit_registers: Optional[float] = None
    occupancy_limit_shared_mem: Optional[float] = None
    occupancy_limit_warps: Optional[float] = None
    registers_per_thread: Optional[float] = None


@dataclass
class PipelineMetrics:
    """Pipeline utilization metrics"""
    fma_pipe_active_pct: Optional[float] = None
    alu_pipe_active_pct: Optional[float] = None
    tensor_pipe_active_pct: Optional[float] = None
    shared_pipe_active_pct: Optional[float] = None
    fp64_pipe_active_pct: Optional[float] = None
    sm_issue_active_pct: Optional[float] = None


@dataclass
class OccupancyMetrics:
    """Occupancy and launch configuration metrics"""
    occupancy_limit_registers: Optional[float] = None
    occupancy_limit_shared_mem: Optional[float] = None
    occupancy_limit_warps: Optional[float] = None
    occupancy_limit_blocks: Optional[float] = None
    waves_per_sm: Optional[float] = None  # Changed from waves_per_multiprocessor to match parser
    block_size: Optional[float] = None
    grid_size: Optional[float] = None
    shared_mem_per_block: Optional[float] = None
    registers_per_thread: Optional[float] = None
    thread_occupancy_pct: Optional[float] = None
    block_occupancy_pct: Optional[float] = None
    warp_occupancy_pct: Optional[float] = None


@dataclass
class ComparisonDeviceMetrics:
    """Device metrics from NCU profiling for kernel comparison (original vs custom)"""
    original_device_metrics: Optional['DeviceMetrics'] = None
    custom_device_metrics: Optional['DeviceMetrics'] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary maintaining the comparison structure expected by tests"""
        result = {}
        if self.original_device_metrics:
            result["original_device_metrics"] = self.original_device_metrics.to_dict()
        if self.custom_device_metrics:
            result["custom_device_metrics"] = self.custom_device_metrics.to_dict()
        # Note: rounding is already applied in DeviceMetrics.to_dict(), but we apply it
        # here as well for consistency and to handle any future float values at this level
        return _round_float_values(result, decimal_places=3)
    
    @classmethod
    def from_dict(cls, d: Dict[str, Any]) -> "ComparisonDeviceMetrics":
        """Create ComparisonDeviceMetrics from dictionary returned by parser for comparisons"""
        original = None
        if "original_device_metrics" in d and d["original_device_metrics"]:
            original = DeviceMetrics.from_dict(d["original_device_metrics"])
        
        custom = None
        if "custom_device_metrics" in d and d["custom_device_metrics"]:
            custom = DeviceMetrics.from_dict(d["custom_device_metrics"])
        
        return cls(
            original_device_metrics=original,
            custom_device_metrics=custom
        )


@dataclass
class DeviceMetrics:
    """Device metrics from NCU profiling, organized by category"""
    speed_of_light: Optional[SpeedOfLightMetrics] = None
    detailed_metrics: Optional[DetailedMetrics] = None
    memory_metrics: Optional[MemoryMetrics] = None
    compute_metrics: Optional[ComputeMetrics] = None
    pipeline_metrics: Optional[PipelineMetrics] = None
    occupancy_metrics: Optional[OccupancyMetrics] = None
    
    def to_dict(self) -> Dict[str, Any]:
        result = {}
        if self.speed_of_light:
            result["speed_of_light"] = _strip_none(asdict(self.speed_of_light))
        if self.detailed_metrics:
            result["detailed_metrics"] = _strip_none(asdict(self.detailed_metrics))
        if self.memory_metrics:
            result["memory_metrics"] = _strip_none(asdict(self.memory_metrics))
        if self.compute_metrics:
            result["compute_metrics"] = _strip_none(asdict(self.compute_metrics))
        if self.pipeline_metrics:
            result["pipeline_metrics"] = _strip_none(asdict(self.pipeline_metrics))
        if self.occupancy_metrics:
            result["occupancy_metrics"] = _strip_none(asdict(self.occupancy_metrics))
        # Round all float values to 3 decimal places
        return _round_float_values(result, decimal_places=3)
    
    @classmethod
    def from_dict(cls, d: Dict[str, Any]) -> "DeviceMetrics":
        """Create DeviceMetrics from dictionary returned by parser"""
        speed_of_light = None
        if "speed_of_light" in d and d["speed_of_light"]:
            speed_of_light = SpeedOfLightMetrics(**d["speed_of_light"])
        
        detailed_metrics = None
        if "detailed_metrics" in d and d["detailed_metrics"]:
            detailed_metrics = DetailedMetrics(**d["detailed_metrics"])
        
        memory_metrics = None
        if "memory_metrics" in d and d["memory_metrics"]:
            memory_metrics = MemoryMetrics(**d["memory_metrics"])
        
        compute_metrics = None
        if "compute_metrics" in d and d["compute_metrics"]:
            compute_metrics = ComputeMetrics(**d["compute_metrics"])
        
        pipeline_metrics = None
        if "pipeline_metrics" in d and d["pipeline_metrics"]:
            pipeline_metrics = PipelineMetrics(**d["pipeline_metrics"])
        
        occupancy_metrics = None
        if "occupancy_metrics" in d and d["occupancy_metrics"]:
            occupancy_metrics = OccupancyMetrics(**d["occupancy_metrics"])
        
        return cls(
            speed_of_light=speed_of_light,
            detailed_metrics=detailed_metrics,
            memory_metrics=memory_metrics,
            compute_metrics=compute_metrics,
            pipeline_metrics=pipeline_metrics,
            occupancy_metrics=occupancy_metrics
        )


@dataclass
class KernelMetadata:
    """Metadata about kernel execution"""
    gpu_id: int
    device_metrics: Optional[Union[DeviceMetrics, ComparisonDeviceMetrics]] = None
    kernel_name: Optional[str] = None
    kernel_type: Optional[str] = None
    gpu_type: Optional[str] = None
    cuda_version: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        result = {"gpu_id": self.gpu_id}
        if self.device_metrics:
            # Both DeviceMetrics and ComparisonDeviceMetrics have to_dict() method
            result["device_metrics"] = self.device_metrics.to_dict()
        if self.kernel_name:
            result["kernel_name"] = self.kernel_name
        if self.kernel_type:
            result["kernel_type"] = self.kernel_type
        if self.gpu_type:
            result["gpu_type"] = self.gpu_type
        if self.cuda_version:
            result["cuda_version"] = self.cuda_version
        return result


@dataclass
class KernelExecutionResult:
    """Result of kernel execution including compilation, validation, and profiling"""
    compiled: bool
    correctness: bool
    runtime: float  # Mean runtime in ms
    metadata: KernelMetadata
    runtime_stats: Optional[RuntimeStats] = None
    compilation_error: Optional[str] = None
    validation_error: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        result = {
            "compiled": self.compiled,
            "correctness": self.correctness,
            "runtime": self.runtime,
            "metadata": self.metadata.to_dict()
        }
        if self.runtime_stats:
            result["runtime_stats"] = {
                "mean": self.runtime_stats.mean,
                "std": self.runtime_stats.std,
                "min": self.runtime_stats.min,
                "max": self.runtime_stats.max,
                "median": self.runtime_stats.median,
                "percentile_95": self.runtime_stats.percentile_95,
                "percentile_99": self.runtime_stats.percentile_99
            }
        if self.compilation_error:
            result["compilation_error"] = self.compilation_error
        if self.validation_error:
            result["validation_error"] = self.validation_error
        return result


# Request/Response Models for HTTP API - Comparison (two kernels)
class CompareRequest(BaseModel):
    """HTTP request model for comparing two kernels"""
    ref_kernel: KernelCode
    custom_kernel: KernelCode

    num_trials: int = 100
    timeout: int = 120
    atol: Optional[float] = 1e-2
    rtol: Optional[float] = 1e-2
    
    def __init__(self, **data):
        """Custom init to handle KernelCode parsing"""
        # Convert kernel dicts to KernelCode instances properly
        if 'ref_kernel' in data and isinstance(data['ref_kernel'], dict):
            data['ref_kernel'] = KernelCode.from_dict(data['ref_kernel'])
        if 'custom_kernel' in data and isinstance(data['custom_kernel'], dict):
            data['custom_kernel'] = KernelCode.from_dict(data['custom_kernel'])
        super().__init__(**data)
    
    class Config:
        """Pydantic config to handle dataclass fields"""
        arbitrary_types_allowed = True


class CompareResponse(BaseModel):
    """HTTP response model for kernel comparison"""
    job_id: str
    kernel_exec_result: KernelExecutionResult
    ref_runtime: RuntimeStats
    pod_name: str
    pod_ip: str
    status: str
    
    class Config:
        """Pydantic config to handle dataclass fields"""
        arbitrary_types_allowed = True


# Request/Response Models for HTTP API - Single Kernel Evaluation
class EvaluationRequest(BaseModel):
    """HTTP request model for evaluating a single kernel"""
    kernel: KernelCode
    
    num_trials: int = 100
    timeout: int = 120
    
    def __init__(self, **data):
        """Custom init to handle KernelCode parsing"""
        # Convert kernel dict to KernelCode instance properly
        if 'kernel' in data and isinstance(data['kernel'], dict):
            data['kernel'] = KernelCode.from_dict(data['kernel'])
        super().__init__(**data)
    
    class Config:
        """Pydantic config to handle dataclass fields"""
        arbitrary_types_allowed = True


class EvaluationResponse(BaseModel):
    """HTTP response model for single kernel evaluation"""
    job_id: str
    kernel_exec_result: KernelExecutionResult
    pod_name: str
    pod_ip: str
    status: str
    
    class Config:
        """Pydantic config to handle dataclass fields"""
        arbitrary_types_allowed = True


class HealthResponse(BaseModel):
    """Health check response model"""
    status: str
    compilation_service: Dict[str, Any]
    profiling_service: Dict[str, Any]


# Internal Service Models
@dataclass
class CompiledKernelInfo:
    """Structured compilation result with kernel-type specific fields"""
    kernel_type: KernelType
    kernel_name: str
    compilation_successful: bool
    
    # Common fields
    gpu_id: int
    
    # TORCH_CUDA specific
    compiled_functions: Optional[Dict[str, Any]] = None  # CuPy RawKernel objects
    cpp_wrapper: Optional[Dict[str, Any]] = None  # Compiled C++ extension
    model_new_source: Optional[str] = None
    cuda_source: Optional[str] = None  # Preprocessed CUDA source
    original_cuda_source: Optional[str] = None
    original_kernel_source: Optional[str] = None
    
    # Error info
    error: Optional[str] = None
    compilation_errors: Optional[List[str]] = None


@dataclass
class CompilationRequest:
    """Request for compilation service with kernel type info"""
    kernel_code: KernelCode
    job_id: Optional[str] = None


@dataclass 
class CompilationResult:
    """Result from compilation service"""
    compiles: bool
    kernel: Optional['BaseExecutableKernel'] = None
    compilation_time: Optional[float] = None
    error: Optional[str] = None


@dataclass
class CompareProfilingRequest:
    """Request for profiling service to compare two kernels"""
    ref_kernel: KernelCode
    custom_kernel: KernelCode
    num_trials: int = 100
    job_id: Optional[str] = None


@dataclass
class CompareProfilingResult:
    """Container for comparison profiling results"""
    success: bool
    original_runtime: Optional[Dict[str, float]] = None
    custom_runtime: Optional[Dict[str, float]] = None
    speedup: float = 0.0
    error: Optional[str] = None
    gpu_id: Optional[int] = None
    metadata: Optional[Dict[str, Any]] = None


@dataclass
class ProfilingRequest:
    """Request for profiling a single kernel"""
    kernel: KernelCode
    num_trials: int = 100
    job_id: Optional[str] = None


@dataclass
class ProfilingResult:
    """Container for single kernel profiling results"""
    success: bool
    runtime_stats: Dict[str, float]
    error: Optional[str] = None
    gpu_id: Optional[int] = None
    metadata: Optional[Dict[str, Any]] = None


# Validation Result Models
@dataclass
class ValidationResult:
    """Result from subprocess validation"""
    is_correct: bool
    trials_passed: int = 0
    total_trials: int = 0
    max_difference: Optional[float] = None
    avg_difference: Optional[float] = None
    error: Optional[str] = None
    execution_time: Optional[float] = None


# Job Management Models
@dataclass
class JobState:
    """Internal job state tracking"""
    job_id: str
    status: str  # submitted, compiling, profiling, completed, failed
    request: Union[EvaluationRequest, CompareRequest]
    created_at: float
    compilation_result: Optional[CompilationResult] = None
    validation_result: Optional[ValidationResult] = None
    profiling_result: Optional[Union[ProfilingResult, CompareProfilingResult]] = None
    result: Optional[EvaluationResponse] = None
    error: Optional[str] = None
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = time.time()


class BaseExecutableKernel(ABC):
    """Context-aware executable kernel that can be passed around and executed"""
    
    def __init__(self, 
                 kernel_type: KernelType,
                 device: torch.device,
                 io_contract: Optional[IOContract] = None):
        """
        Initialize executable kernel
        
        Args:
            kernel_type: Type of kernel (TORCH, CUDA, TORCH_CUDA, etc.)
            device: CUDA device to execute on
            io_contract: Optional I/O specifications
        """
        self.kernel_type = kernel_type
        self.device = device
        self.input_specs = io_contract.args if io_contract and io_contract.args else None
        
        # Execution context
        self._default_inputs = None
        self._profiling_mode = False
        self._use_cuda_graphs = False
        
        # Initialize kernel-specific components
        self._initialize_kernel()
    
    @abstractmethod
    def _initialize_kernel(self):
        """Initialize kernel-specific components"""
        pass
        
    def with_inputs(self, *inputs):
        """Set default inputs (fluent interface)"""
        self._default_inputs = inputs
        return self
        
    def with_profiling(self, use_cuda_graphs=False):
        """Enable profiling mode"""
        self._profiling_mode = True
        self._use_cuda_graphs = use_cuda_graphs
        return self
        
    def __call__(self, *inputs):
        """Make the kernel directly callable"""
        return self._execute_impl(*inputs)
        
    @abstractmethod
    def _execute_impl(self, *inputs) -> Optional[Any]:
        """Actual execution implementation"""
        pass